---
title: "The Usage of Moral Categories in Academic Philosophy"
date: "November 5th, 2017"
---

A *long* time ago, something called morality entered into human consiousness,
presumably, beaten into our long-dead ancestors to enforce rules formerly backed only in violence.
It occupies a layer between our emotional reactions to certain decisions,
and our actual ability to make decisions.
Defined in a philosophic terms, it's a mental system used to determine the value of decisions,
and thus, to make decisions.

Of course, that's not quite the whole story;
there are strong emotional connotations involved with this value-assignment,
happiness and fullfillment when we live up to the ideals of our moral system,
disgust when actions contrary to it are performed.
Such systems are also social, in the sense that you use them to evaluate
the decisions of others, to determine the value of the person.

Leaving aside the questions about how useful that particular assumption is,
the study of these systems (and the categories I already formed by describing them to you) falls under the purview of
Philosophy, this study being one of the pillars of the field.

They have a lot of terminology to describe the things I'm about to describe,
none of which I'm going to use, in the interest of seperating the connotation from the definition.

It should also be noted that the following little narrative is a complete fabrication.

## storytime

Philosophers, in their quest to understanding the nature of morality, started looking for patterns in moral systems.
So they formed an experimental procedure of sorts, a way to explore the moral systems of people,
based on asking questions on moral topics, such as killing or theft.

"Is killing wrong? What about soldiers? What about self-defense?"

"Is theft wrong? Would you steal something if you really needed it? To feed your starving family?"

Based on these answers, they came up with a number of categories and subcategories, of which, for brevity's sake, I will only mention two.

The first is the rule-makers, who tend to form simple rules in response to moral queries, and take actions which don't violate these rules:
"Killing is immoral, unless it's in self-defense."

The second is the number-crunchers, who place values, usually numberical, on consequences, and compare them to determine what to do.
They look at the future where they don't act, and see five people dead, then look at the future where they do act,
and see only one person dead. So they act.

Ideas, once given names, rarely stay dormant.
The philosophers who studied that patterns incorperated them into their identities, becoming rule-makers and number-crunchers,
taking these systems as their own morals, played out like robots.
They argued, the rule-makers pointing out that the number-crunchers would commit atrocities to one in order to save the lives of two,
and the number-crunchers responding by saying the rule-makers would sooner die of hunger than steal a loaf of bread.
Through this process, they refined their systems, creating categories upon categories, opining on guilt, blame, and responsibility,
until the system was too convoluted for the unlearned to use or understand.

## systemization

In general, I have no problem with models of the same type as moral categories.
Freud's theories of id, ego, and superego are, despite the dismal epistemology and disturbing familial aspects,
extremely useful in helping humans understand the nature of the mind, which is largely the point of models.
Same for the economic model of supply and demand curves or the point-masses of physics.
I chose these examples because it's relatively easy to see that *they are not the thing they represent*,
the map is not the territory.

This is easy to see in the 'cracks' in the theories, as in [Nozick's utility monster](https://en.wikipedia.org/wiki/Utility_monster),
or the fact that utility calculations on the value of a human life don't usually take into account the value of that life to the person doing
the calculating, which leads to things like "I have so internalized utility, I would sacrifice the life of my best friend to save two other people."
On the deontological side, the fact that people break moral rules all the time, which calls into question how useful they are anyway,
and the fact that, when given a decision between two immoral actions, one will be chosen, which implies a partial ordering to moral rules.

Remember the context in which these models were made.
The cracks are the places where the models falter and normal human decision making takes control.
They do not describe the entirety of human decision making, nor should they.
Nozick is merely pointing out a discrepency between how the model would work if you programmed some robots to go by it alone, and how humans actually behave.
Not that this kind of question isn't useful, in the proper context, but as a "criticism of utilitaranism", not so much.
They seem more like "gotchas" than attempts to make distinctions or points about the system, or to advance it by building up the system.

Essentially, this gives philosophers a new way to respond to such criticisms:
"yes, that could be used to refine the model, but it's too complicated to fit in the core theory", or
"this is too far on the usefulness/complexity curve to be worth teaching or learning about."
Hopefully, this will help refine the categories of moral philosophy somewhat,
which are currently nebulous to the point they don't deserve to be called categories.
It's not like I want a whole hierarchy of useless information,
but a list of assumptions that define each category, e.g. "this assumes you use the same moral standards for everyone",
"this only takes into account things which are both knowable and beneficial to society", would be extremely useful.

If you're one of those people who thinks that following rules is good in and of itself,
and want to build up actual rule systems, not categories, whatever man, just don't mix that up with the categories.
Also if you're trying to build up a literal human intelligence through algorithmic utility calculations.
As a side note, if you think the law follows such rules, and that's what makes it just, you have it completely backwards.
Laws are largely composed of rules because it wouldn't make sense any other way, their role is to specify which actions are illegal,
which is hard to do from the consequences alone, although such considerations do leak into sentencing; laws tend to follow moral rules
because people are happy when the laws reflect their morals, and the rule of law exists to hold society together.

More succinctly, I think it's possible to create moral categories that still allow for personal moral explanation,
without pouring over every little detail that would be in a fully algorithmic system,
and without thinking you need pick elements from only the rule-maker or number-cruncher side.
Ideally, teaching that these systems were intented for different minds and historical contexts,
while leaving room for rhetorical development.
